{
	"name": "DataInjestion",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "RetailSales",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "62df8ff4-41eb-4c5a-9d82-c7c24d3f196a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/bcf8d11e-de59-4024-bb63-ea1da01a08c4/resourceGroups/Retailsales/providers/Microsoft.Synapse/workspaces/retailsalesws/bigDataPools/RetailSales",
				"name": "RetailSales",
				"type": "Spark",
				"endpoint": "https://retailsalesws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/RetailSales",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"# Import necessary libraries\r\n",
					"import os\r\n",
					"import time\r\n",
					"import subprocess\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from pyspark.sql import SparkSession"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Set up Kaggle API credentials\r\n",
					"time.sleep(60) \r\n",
					"\r\n",
					"# Initialize Spark session\r\n",
					"spark = SparkSession.builder.appName(\"DataIngestion\").getOrCreate()\r\n",
					"\r\n",
					"# Ensure Spark session is initialized\r\n",
					"if not spark:\r\n",
					"    raise Exception(\"Failed to initialize Spark session\")\r\n",
					"\r\n",
					"\r\n",
					"os.environ['KAGGLE_USERNAME'] = \"techcoder09\"\r\n",
					"kaggle_dataset = \"thedevastator/unlock-profits-with-e-commerce-sales-data\"\r\n",
					"download_path = '/tmp/kaggle_data'\r\n",
					"os.makedirs(download_path, exist_ok=True)\r\n",
					"\r\n",
					"# Function to download dataset from Kaggle with retries\r\n",
					"def download_kaggle_dataset(kaggle_dataset, download_path, retries):\r\n",
					"    for attempt in range(retries):\r\n",
					"        # Download dataset from Kaggle\r\n",
					"        process = subprocess.run(['kaggle', 'datasets', 'download', '-d', kaggle_dataset, '-p', download_path], \r\n",
					"                                 stdout=subprocess.PIPE, stderr=subprocess.PIPE)\r\n",
					"        # Print output and errors for debugging\r\n",
					"        print(\"STDOUT:\", process.stdout.decode('utf-8'))\r\n",
					"        print(\"STDERR:\", process.stderr.decode('utf-8'))\r\n",
					"\r\n",
					"        # Verify if the file was downloaded correctly\r\n",
					"        downloaded_file_path = os.path.join(download_path, kaggle_dataset.split('/')[-1] + '.zip')\r\n",
					"        if os.path.exists(downloaded_file_path) and os.path.getsize(downloaded_file_path) > 0:\r\n",
					"            print(\"File downloaded successfully.\")\r\n",
					"            return downloaded_file_path\r\n",
					"        else:\r\n",
					"            print(f\"Failed to download the file or the file is empty. Retrying... ({attempt + 1}/{retries})\")\r\n",
					"            time.sleep(60)  # Wait before retrying\r\n",
					"\r\n",
					"    raise Exception(\"Failed to download the file after multiple attempts.\")\r\n",
					"\r\n",
					"# Download dataset\r\n",
					"try:\r\n",
					"    downloaded_file_path = download_kaggle_dataset(kaggle_dataset, download_path,3)\r\n",
					"except Exception as e:\r\n",
					"    print(str(e))"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\r\n",
					"\"\"\"\r\n",
					"# Kaggle API setup\r\n",
					"\r\n",
					"# Set up Kaggle API credentials (make sure kaggle.json is available in ~/.kaggle or set up environment variables)\r\n",
					"os.environ['KAGGLE_USERNAME'] = \"techcoder09\"\r\n",
					"kaggle_dataset = \"thedevastator/unlock-profits-with-e-commerce-sales-data\"\r\n",
					"download_path = '/tmp/kaggle_data'\r\n",
					"os.makedirs(download_path, exist_ok=True)\r\n",
					"\r\n",
					"# Download dataset from Kaggle\r\n",
					"subprocess.run(['kaggle', 'datasets', 'download', '-d', kaggle_dataset, '-p', download_path])\r\n",
					"\r\n",
					"# Print output and errors\r\n",
					"#print(\"STDOUT:\", process.stdout.decode('utf-8'))\r\n",
					"#print(\"STDERR:\", process.stderr.decode('utf-8'))\r\n",
					"\r\n",
					"# Verify if the file was downloaded correctly\r\n",
					"downloaded_file_path = os.path.join(download_path, kaggle_dataset.split('/')[-1] + '.zip')\r\n",
					"if os.path.exists(downloaded_file_path) and os.path.getsize(downloaded_file_path) > 0:\r\n",
					"    print(\"File downloaded successfully.\")\r\n",
					"else:\r\n",
					"    print(\"Failed to download the file or the file is empty.\")\r\n",
					"\"\"\"\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Retrieve the storage account key securely\r\n",
					"storage_account_name = mssparkutils.credentials.getSecret(\"retailsaleskeyvault\", \"RetailSalesStorageAccountName\")\r\n",
					"container_name = mssparkutils.credentials.getSecret(\"retailsaleskeyvault\", \"RetailCintainer\")\r\n",
					"blob_name = os.path.basename(downloaded_file_path)\r\n",
					"account_key = mssparkutils.credentials.getSecret(\"retailsaleskeyvault\", \"RetailSalesStorageaccountkey\")\r\n",
					"\r\n",
					"# Create a connection to Azure Blob Storage\r\n",
					"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
					"\r\n",
					"connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\r\n",
					"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n",
					"blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\r\n",
					"\r\n",
					"# Upload the file to Blob Storage\r\n",
					"with open(downloaded_file_path, \"rb\") as data:\r\n",
					"    blob_client.upload_blob(data, overwrite=True)\r\n",
					"    print(f\"File {blob_name} uploaded to Azure Blob Storage.\")"
				],
				"execution_count": 33
			}
		]
	}
}